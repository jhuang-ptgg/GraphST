
╭─── Claude Code v2.1.34 ──────────────────────────────────────────────────────╮
│                                                    │ Tips for getting        │
│                 Welcome back Jason!                │ started                 │
│                                                    │ ✔ Run /init to create … │
│                       ▐▛███▜▌                      │ ─────────────────────── │
│                      ▝▜█████▛▘                     │ Recent activity         │
│                        ▘▘ ▝▝                       │ No recent activity      │
│                                                    │                         │
│  Opus 4.6 · Claude Pro · jasonhuang@ptglab.com's   │                         │
│  Organization                                      │                         │
│                 ~/gitplaza/GraphST                 │                         │
╰──────────────────────────────────────────────────────────────────────────────╯

╭──────────────────────────────────────────────────────────────────────────────╮
│ Plan to implement                                                            │
│                                                                              │
│ Plan: Modernize GraphST with Dask Support for ~1M Spots                      │
│                                                                              │
│ Context                                                                      │
│                                                                              │
│ GraphST is a spatial transcriptomics tool that uses graph convolutional      │
│ networks for spatial domain identification and cell-type deconvolution. The  │
│ current implementation processes all data as a single batch with dense NxN   │
│ matrices, limiting it to ~50K spots. For modern datasets approaching 1M      │
│ spots (e.g., Stereo-seq, MERFISH), the memory requirements are intractable:  │
│                                                                              │
│ - NxN adjacency/distance matrices: 1M x 1M = 4TB (dense float32)             │
│ - Feature matrix: 1M x 3000 genes = 12GB when densified                      │
│ - Full-batch GPU loading: All tensors loaded at once                         │
│                                                                              │
│ This plan introduces Dask-based chunked preprocessing, sparse-only graph     │
│ construction, and mini-batch GCN training across 5 phases.                   │
│                                                                              │
│ ---                                                                          │
│ Phase 1: Chunked Sparse Graph Construction                                   │
│                                                                              │
│ The single biggest bottleneck. Both construct_interaction() and              │
│ construct_interaction_KNN() in preprocess.py allocate dense NxN arrays.      │
│                                                                              │
│ New file: GraphST/chunked_graph.py                                           │
│                                                                              │
│ def construct_interaction_chunked(adata, n_neighbors=3, chunk_size=50000,    │
│ n_jobs=-1):                                                                  │
│     """Build sparse KNN graph without dense NxN allocation."""               │
│                                                                              │
│ Strategy:                                                                    │
│ 1. Build a single sklearn.neighbors.NearestNeighbors index on all 1M spatial │
│  coordinates (only 1M x 2 = 8MB — fits easily in memory)                     │
│ 2. Query the index in chunks of chunk_size rows using dask.delayed for       │
│ parallelism                                                                  │
│ 3. Each chunk returns (row_indices, col_indices) pairs                       │
│ 4. Assemble into a global scipy.sparse.csr_matrix — never allocate NxN dense │
│ 5. Symmetrize in sparse format: adj = adj + adj.T; adj.data =                │
│ np.clip(adj.data, 0, 1)                                                      │
│ 6. Store adata.obsm['adj'] and adata.obsm['graph_neigh'] as scipy sparse     │
│ matrices                                                                     │
│                                                                              │
│ Modify: GraphST/preprocess.py                                                │
│                                                                              │
│ - Add routing: if adata.n_obs > threshold, call                              │
│ construct_interaction_chunked instead of existing functions                  │
│ - preprocess_adj_sparse() (line 152-158) already handles sparse input — no   │
│ changes needed                                                               │
│ - Ensure preprocess_adj() branches to sparse path for large data             │
│                                                                              │
│ Modify: GraphST/GraphST.py (__init__)                                        │
│                                                                              │
│ - Line 115: graph_neigh currently loaded as dense FloatTensor. For large     │
│ data, use sparse_mx_to_torch_sparse_tensor() instead                         │
│                                                                              │
│ ---                                                                          │
│ Phase 2: Dask-Based Chunked Feature Preprocessing                            │
│                                                                              │
│ New file: GraphST/dask_preprocess.py                                         │
│                                                                              │
│ def get_feature_chunked(adata, chunk_size=50000, deconvolution=False):       │
│     """Extract features as dask arrays with row-wise chunking.               │
│     Avoids .toarray() on the full sparse matrix."""                          │
│                                                                              │
│ def permutation_chunked(feature_dask_array):                                 │
│     """Permute rows of a dask array via dask.array.map_blocks."""            │
│                                                                              │
│ Strategy:                                                                    │
│ 1. Keep adata.X sparse — avoid the .toarray() call at GraphST.py line        │
│ 132-146                                                                      │
│ 2. Convert to dask.array with row-wise chunks:                               │
│ dask.array.from_array(sparse_matrix, chunks=(chunk_size, n_genes))           │
│ 3. sc.pp.highly_variable_genes, normalize_total, log1p — these work on       │
│ sparse AnnData already, no changes needed                                    │
│ 4. sc.pp.scale(zero_center=False) — column-wise, works on sparse without     │
│ densifying                                                                   │
│ 5. Augmented features (feat_a): apply permutation as dask.array.map_blocks   │
│ 6. Contrastive labels (add_contrastive_label): only 1M x 2 = 8MB, keep as-is │
│  (no chunking needed)                                                        │
│                                                                              │
│ AnnData compatibility: AnnData >= 0.8 supports Dask arrays in .obsm. Verify  │
│ and pin minimum version.                                                     │
│                                                                              │
│ ---                                                                          │
│ Phase 3: Mini-Batch GCN Training (Most Complex)                              │
│                                                                              │
│ The GCN forward pass z = adj @ (feat @ weight) couples neighboring nodes, so │
│  naive row-chunking is incorrect. Use ClusterGCN-style spatial partitioning. │
│                                                                              │
│ New file: GraphST/graph_sampler.py                                           │
│                                                                              │
│ class SpatialClusterSampler:                                                 │
│     """ClusterGCN-style sampler for spatial graphs.                          │
│                                                                              │
│     Partitions spots into spatial grid clusters. Each mini-batch             │
│     includes a cluster plus its 1-hop boundary neighbors.                    │
│     """                                                                      │
│     def __init__(self, adj_sparse, positions, features_dask,                 │
│                  cluster_size=10000, n_hops=1): ...                          │
│                                                                              │
│     def __iter__(self):                                                      │
│         """Yields (sub_features, sub_features_a, sub_adj,                    │
│                   sub_graph_neigh, sub_label_CSL, node_indices)"""           │
│                                                                              │
│ Partitioning strategy:                                                       │
│ - Divide spatial domain into rectangular grid cells, each ~5K-20K spots      │
│ - For each mini-batch: select a cluster, expand by 1-hop neighbors (from     │
│ sparse adj), extract sub-adjacency and feature slices                        │
│ - Dask array slicing → .compute() → torch.FloatTensor at the sampler         │
│ boundary                                                                     │
│                                                                              │
│ Modify: GraphST/model.py                                                     │
│                                                                              │
│ - Encoder_sparse.forward(): Accept graph_neigh as a forward-pass argument    │
│ (not just constructor param) so it can vary per batch                        │
│ - AvgReadout.forward(): Use torch.spmm(mask, emb) when mask is sparse        │
│ (currently torch.mm)                                                         │
│                                                                              │
│ Modify: GraphST/GraphST.py (train())                                         │
│                                                                              │
│ - Add mini-batch training loop alongside existing full-batch path:           │
│ if self.large_scale:                                                         │
│     for epoch in range(self.epochs):                                         │
│         for batch in sampler:                                                │
│             sub_feat, sub_adj, ... = [t.to(device) for t in batch]           │
│             loss = forward_and_compute_loss(sub_feat, sub_adj, ...)          │
│             loss.backward()                                                  │
│             optimizer.step()                                                 │
│             optimizer.zero_grad()                                            │
│ else:                                                                        │
│     # existing full-batch code, unchanged                                    │
│ - Post-training: compute embeddings in chunks (no-grad forward pass over     │
│ sampler), assemble into adata.obsm['emb']                                    │
│                                                                              │
│ Why ClusterGCN? GraphST's GCN is 1-layer deep (single adj @ z), so 1-hop     │
│ boundary captures all dependencies. Spatial locality means clusters have     │
│ minimal boundary overlap. Simpler than PyG's neighbor sampling.              │
│                                                                              │
│ ---                                                                          │
│ Phase 4: Scale Contrastive Loss and Deconvolution                            │
│                                                                              │
│ Contrastive loss (Noise_Cross_Entropy in GraphST.py)                         │
│                                                                              │
│ - Currently computes pred_sp @ emb_sp.T (NxN cosine similarity matrix)       │
│ - With ClusterGCN batching, this is computed within each mini-batch (~10K x  │
│ 10K = manageable)                                                            │
│ - Modify to accept batch-local graph_neigh for positive pair masking         │
│                                                                              │
│ Deconvolution (Encoder_map in model.py)                                      │
│                                                                              │
│ - Current: learnable (n_cell, n_spot) matrix = 10K x 1M = 40GB               │
│ - Replace with low-rank: M_approx = U @ V.T where U: (n_cell, rank), V:      │
│ (n_spot, rank), rank ~64-128                                                 │
│ - New class: Encoder_map_lowrank in model.py                                 │
│                                                                              │
│ Label refinement (refine_label in utils.py)                                  │
│                                                                              │
│ - Currently calls ot.dist(position, position) — another NxN dense matrix     │
│ - Replace with pre-computed KNN graph from Phase 1 (already have k=3         │
│ neighbors; optionally rebuild with larger k)                                 │
│                                                                              │
│ ---                                                                          │
│ Phase 5: Backward Compatibility and API                                      │
│                                                                              │
│ API changes to GraphST.py constructor:                                       │
│                                                                              │
│ class GraphST():                                                             │
│     def __init__(self, adata, ...,                                           │
│                  large_scale=None,     # None = auto-detect based on n_obs   │
│                  chunk_size=50000,     # rows per chunk for Dask/KNN         │
│                  batch_size=10000,     # spots per training mini-batch       │
│                  ):                                                          │
│ - large_scale=None: auto-detect (threshold: 100K spots)                      │
│ - large_scale=False: original code path, completely unchanged                │
│ - large_scale=True: chunked/batched path                                     │
│                                                                              │
│ Dependency updates (setup.py):                                               │
│                                                                              │
│ install_requires = [                                                         │
│     "requests",                                                              │
│     "dask[array]",                                                           │
│     "pynndescent",      # fast approximate KNN (optional, fallback to        │
│ sklearn)                                                                     │
│ ]                                                                            │
│                                                                              │
│ ---                                                                          │
│ File Change Summary                                                          │
│ File: GraphST/chunked_graph.py                                               │
│ Action: NEW                                                                  │
│ Description: Chunked KNN graph construction with Dask parallelism, outputs   │
│   scipy sparse                                                               │
│ ────────────────────────────────────────                                     │
│ File: GraphST/dask_preprocess.py                                             │
│ Action: NEW                                                                  │
│ Description: Dask-wrapped feature extraction, chunked permutation            │
│ ────────────────────────────────────────                                     │
│ File: GraphST/graph_sampler.py                                               │
│ Action: NEW                                                                  │
│ Description: SpatialClusterSampler for ClusterGCN mini-batching              │
│ ────────────────────────────────────────                                     │
│ File: GraphST/preprocess.py                                                  │
│ Action: MODIFY                                                               │
│ Description: Route to chunked functions for large data; keep existing        │
│ functions                                                                    │
│    intact                                                                    │
│ ────────────────────────────────────────                                     │
│ File: GraphST/model.py                                                       │
│ Action: MODIFY                                                               │
│ Description: AvgReadout → sparse support; Encoder_sparse → per-batch         │
│   graph_neigh; add Encoder_map_lowrank                                       │
│ ────────────────────────────────────────                                     │
│ File: GraphST/GraphST.py                                                     │
│ Action: MODIFY                                                               │
│ Description: Add large_scale/chunk_size/batch_size params; mini-batch train  │
│   loop; chunked embedding assembly; batch-local contrastive loss             │
│ ────────────────────────────────────────                                     │
│ File: GraphST/utils.py                                                       │
│ Action: MODIFY                                                               │
│ Description: refine_label() → KNN-based instead of ot.dist                   │
│ ────────────────────────────────────────                                     │
│ File: GraphST/__init__.py                                                    │
│ Action: MODIFY                                                               │
│ Description: Export new public functions                                     │
│ ────────────────────────────────────────                                     │
│ File: setup.py                                                               │
│ Action: MODIFY                                                               │
│ Description: Add dask[array], pynndescent dependencies                       │
│ ---                                                                          │
│ Implementation Order                                                         │
│                                                                              │
│ 1. Phase 1 — Chunked graph construction (highest priority, unblocks          │
│ everything)                                                                  │
│ 2. Phase 2 — Dask feature preprocessing (can parallel with Phase 1)          │
│ 3. Phase 3 — Mini-batch training (depends on 1 & 2, most complex)            │
│ 4. Phase 4 — Contrastive loss + deconvolution scaling (depends on 3)         │
│ 5. Phase 5 — Backward compatibility testing throughout                       │
│                                                                              │
│ ---                                                                          │
│ Verification Plan                                                            │
│                                                                              │
│ 1. Unit tests: Verify chunked graph construction produces identical sparse   │
│ adjacency to original dense version on small data (~1K spots)                │
│ 2. Numerical equivalence: Run original vs large-scale path on a 5K-spot      │
│ dataset, compare embeddings (should be close but not identical due to        │
│ mini-batch vs full-batch)                                                    │
│ 3. Memory profiling: Use tracemalloc / nvidia-smi to confirm peak memory     │
│ stays under 16GB for 100K+ spots                                             │
│ 4. Scale test: Run on a synthetic 1M-spot dataset (random coordinates +      │
│ sparse count matrix) to verify end-to-end completion                         │
│ 5. Clustering quality: Compare ARI/NMI on benchmark datasets (e.g., DLPFC    │
│ 151673) between original and large-scale paths                               │
╰──────────────────────────────────────────────────────────────────────────────╯

● I'll start by reading the existing codebase to understand the current
  implementation before making changes.

● Read 6 files (ctrl+o to expand)

● Now I have a thorough understanding of the codebase. Let me create the task
  list and start implementing.
  ⎿  Interrupted · What should Claude do instead?

❯ /init 

● Let me explore the codebase more thoroughly to understand the structure.

● Wrote 1 memory, searched for 6 patterns, read 3 files (ctrl+o to expand)

● Created CLAUDE.md with:

  - Project overview — what GraphST does and its publication context
  - Setup — install command and key dependencies
  - Architecture — the 4-module pipeline (preprocess.py → model.py → GraphST.py
  → utils.py) with key functions documented
  - Data flow — how adata.obsm slots connect the pipeline stages
  - Data type branching — the '10X' vs 'Stereo'/'Slide' code paths
  - Scalability constraints — the dense NxN bottlenecks that matter for future
  work

✻ Cooked for 48s